{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e63f01-296a-449d-9fb3-70aedf7a4b05",
   "metadata": {},
   "source": [
    "# 7.2.2. Multi Layer Perception Model\n",
    "\n",
    "1. See Notion Notes for short-hand notation, explanations, my thoughts, etc.\n",
    "2. BOOKS:\n",
    "    - Deep Learning for Time Series Forecasting - Predict the Future with MLPs, CNNs and LSTMs in Python by Jason Brownlee\n",
    "    - Introduction to TSF with Python - How to Prepare Data and Develop Models to Predict the Future by Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5754059-f73d-4f20-9cbd-8f27e16cefd7",
   "metadata": {},
   "source": [
    "# Imports + Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd53d4ac-3728-4634-9b56-6a4d9e9ec10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:42:41.304864: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce953da-c619-4680-8cc0-0d2ecbc63700",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/Users/brinkley97/Documents/development/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7fccdd-aa53-4236-adcd-8993a31a3bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/brinkley97/Documents/development/book-intro_to_tsf_with_python/3-timeSeriesAsSupervisedLearning.ipynb'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_BOOK = 'book-intro_to_tsf_with_python/'\n",
    "TS_AS_SML_FILE = BASE + PATH_TO_BOOK + '3-timeSeriesAsSupervisedLearning.ipynb'\n",
    "TS_AS_SML_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9f7a62-e15f-481b-b7dd-29dd97179473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load TS_AS_SML_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a8691f-353a-468b-8d05-d9df240d79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run $TS_AS_SML_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998cf99-ab26-4b18-b031-46aca11a3312",
   "metadata": {},
   "source": [
    "# For Full Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732a2b22-b84a-4d8f-b565-bf22289cc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BASE + PATH_TO_BOOK + 'datasets/daily-min-temperatures.csv'\n",
    "series = pd.read_csv(dataset, header=0, index_col=0)\n",
    "# series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4101ccc-0796-4e17-884b-38e65b429c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.7],\n",
       "       [17.9],\n",
       "       [18.8],\n",
       "       ...,\n",
       "       [13.5],\n",
       "       [15.7],\n",
       "       [13. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = series.values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65a60cb-7c6b-47d4-a28f-408156d015b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets \n",
    "def train_test_split(data, n_test):\n",
    "    print(len(data[:-n_test]))\n",
    "    print(len(data[-n_test:]))\n",
    "    return data[:-n_test], data[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3408f56-cd11-4b93-8ab5-5a108078ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(specific_features):\n",
    "#     \"\"\"Split data into TRAIN, TEST, and VALIDATION sets\n",
    "    \n",
    "#     Parameters:\n",
    "#     specific_features -- list (of the reduced features)\n",
    "#     specific_features_true_labels -- list (of the emotion labels that belong to that specifuc feature)\n",
    "#     test_size -- float (to pass into sklearn train_test_split())\n",
    "    \n",
    "#     Return:\n",
    "#     X_train, X_test, X_val, y_train, y_test, y_val -- list (for that specific subset of the features)\n",
    "    \n",
    "#     \"\"\"\n",
    "#     print(specific_features)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(specific_features, test_size=0.2, random_state=1)\n",
    "#     total_X = len(X_train) + len(X_test)\n",
    "#     total_Y = len(y_train) + len(y_test)\n",
    "    \n",
    "#     print(\"[INFO] X, y TRAIN sets\")\n",
    "#     print(np.shape(X_train), np.shape(y_train))\n",
    "    \n",
    "#     print(\"\\n[INFO] X, y TEST sets\")\n",
    "#     print(np.shape(X_test), np.shape(y_test))\n",
    "#     # print(\"[INFO] TOTAL TRAIN, TEST sets\")\n",
    "#     # print(total_X, total_Y)\n",
    "    \n",
    "#     return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b79fddcd-5aa5-41c4-9d24-47aff832d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3638\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# data split \n",
    "n_test = 12 \n",
    "training_data, test_data = train_test_split(data, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b73440d-ee9e-41ba-b67c-5824526585bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef7d5da-e824-4e68-89dd-4b49cda26b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_uts_sequence_to_sml(uts_observations, prior_observations, forecasting_step):\n",
    "    \"\"\"Splits a given UTS into multiple input rows where each input row has a specified number of timestamps and the output is a single timestamp.\n",
    "    \n",
    "    Parameters:\n",
    "    uts_observations -- 1D np array (of UTS data to transform to SML data with size  b rows/length x 1 dimension)\n",
    "    prior_observations -- py int (of all observations before we get to where we want to start making the predictions)\n",
    "    forecasting_step -- py int (of how far out to forecast, 1 only the next timestamp, 2 the next two timestamps, ... n the next n timestamps)\n",
    "    \n",
    "    Return:\n",
    "    agg.values -- np array (of new sml data)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(uts_observations)\n",
    "    cols = list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for prior_observation in range(prior_observations, 0, -1):\n",
    "        cols.append(df.shift(prior_observation))\n",
    "    \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, forecasting_step):\n",
    "        cols.append(df.shift(-i))\n",
    "        \n",
    "        # put it all together\n",
    "        agg = pd.concat(cols, axis=1) \n",
    "        \n",
    "        # drop rows with NaN values\n",
    "        agg.dropna(inplace=True)\n",
    "    # print(agg)\n",
    "    \n",
    "    return agg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa674a1-f029-4930-8540-02d0c818a6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.7, 17.9, 18.8, ..., 14.4, 16. , 16.5],\n",
       "       [17.9, 18.8, 14.6, ..., 16. , 16.5, 18.7],\n",
       "       [18.8, 14.6, 15.8, ..., 16.5, 18.7, 19.4],\n",
       "       ...,\n",
       "       [10.5, 11.1, 13. , ..., 13.4, 13.6, 13.9],\n",
       "       [11.1, 13. , 12.9, ..., 13.6, 13.9, 17.2],\n",
       "       [13. , 12.9,  8.8, ..., 13.9, 17.2, 14.7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_observations, forecasting_step = [24, 1]\n",
    "converted_uts_to_sml = convert_uts_sequence_to_sml(training_data, prior_observations, forecasting_step) \n",
    "converted_uts_to_sml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da098a26-e54d-4380-a89d-b50062591c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model \n",
    "def mlp_model_fit(sml_data, n_nodes, n_epochs, n_batch, prior_observations): \n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    sml_data -- \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare data \n",
    "    # converted_uts_to_sml = convert_uts_sequence_to_sml(train, prior_observations, forecasting_step) \n",
    "    print(sml_data)\n",
    "    \n",
    "    train_x, train_y = sml_data[:, :-1], sml_data[:, -1] \n",
    "    # define model\n",
    "    model = Sequential() \n",
    "    model.add(Dense(n_nodes, activation='relu' , input_dim=prior_observations)) \n",
    "    model.add(Dense(1)) \n",
    "    model.compile(loss='mse' , optimizer='adam') \n",
    "    \n",
    "    # fit\n",
    "    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edd124fe-f3f6-46f4-a53b-557e5ae6aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[20.7 17.9 18.8 ... 14.4 16.  16.5]\n",
      " [17.9 18.8 14.6 ... 16.  16.5 18.7]\n",
      " [18.8 14.6 15.8 ... 16.5 18.7 19.4]\n",
      " ...\n",
      " [10.5 11.1 13.  ... 13.4 13.6 13.9]\n",
      " [11.1 13.  12.9 ... 13.6 13.9 17.2]\n",
      " [13.  12.9  8.8 ... 13.9 17.2 14.7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:42:44.060367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "n_nodes, n_epochs, n_batch = [500, 100, 100] \n",
    "print(n_epochs)\n",
    "model = mlp_model_fit(converted_uts_to_sml, n_nodes, n_epochs, n_batch, prior_observations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ef6d103-0507-4f2e-986d-6caade15342e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x14d21d100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d487b64-4a94-401c-a736-eea3c770579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2be572bd-5100-492d-b2b9-14b8273afe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast with a pre-fit model \n",
    "def model_predict(model, history, n_input): \n",
    "    # unpack config \n",
    "    # n_input, _, _, _ = config \n",
    "    # prepare data \n",
    "    x_input = np.array(history[-n_input:]).reshape(1, n_input) \n",
    "    # forecast \n",
    "    yhat = model.predict(x_input, verbose=0) \n",
    "    \n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c76a73a-37e8-4c4c-ad82-663431a5570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation for univariate data \n",
    "def walk_forward_validation(training_data, n_test, model, cfg, test_data): \n",
    "    predictions = list()\n",
    "    \n",
    "    # split dataset \n",
    "    # train, test = train_test_split(data, n_test) \n",
    "    # fit model \n",
    "    # model = model_fit(train, cfg) \n",
    "    # seed history with training dataset \n",
    "    history = [x for x in training_data]\n",
    "    \n",
    "    # step over each time-step in the test set \n",
    "    for i in range(len(test_data)): \n",
    "        # fit model and make forecast for history \n",
    "        yhat = model_predict(model, history, cfg) \n",
    "        \n",
    "        # store forecast in list of predictions \n",
    "        predictions.append(yhat) \n",
    "        \n",
    "        # add actual observation to history for the next loop \n",
    "        history.append(test_data[i])\n",
    "        \n",
    "    # estimate prediction error \n",
    "    error = measure_rmse(test_data, predictions) \n",
    "    print('> %.3f' % error) \n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93889736-96f2-4f61-a46b-95e9df5199b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1.672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6715478050695394"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_forward_validation(training_data, n_test, model, prior_observations, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47526d01-8610-4dbf-8e53-a65dd1396752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat evaluation of a config \n",
    "def repeat_evaluate(training_data, n_test, model, prior_observations, n_repeats=30): \n",
    "    \n",
    "    # fit and evaluate the model n times \n",
    "    scores = [walk_forward_validation(training_data, n_test, model, prior_observations, test_data) for _ in range(n_repeats)]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26939aa4-2b4a-4bae-8060-3e7fc9fc1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize model performance\n",
    "def summarize_scores(name, scores): \n",
    "    # print a summary\n",
    "    scores_m, score_std = mean(scores), std(scores)\n",
    "    print(' %s: %.3f RMSE (+/- %.3f)' % (name, scores_m, score_std)) \n",
    "    \n",
    "    # box and whisker plot \n",
    "    pyplot.boxplot(scores)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42b2dbdb-e180-4cf8-a18c-bbbfbe44878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df41ce63-e34c-4c9d-ac82-45d3d461c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = [24, 500, 100, 100] \n",
    "# model = model_fit(train, cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "811441c6-1d10-4e1f-aa15-fa1bc7fa8f2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # define config\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# config = [24, 500, 100, 100] \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# # grid search \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# scores = repeat_evaluate(data, config, n_test)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m, in \u001b[0;36mrepeat_evaluate\u001b[0;34m(training_data, n_test, model, prior_observations, n_repeats)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat_evaluate\u001b[39m(training_data, n_test, model, prior_observations, n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m): \n\u001b[1;32m      3\u001b[0m     \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# fit and evaluate the model n times \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [walk_forward_validation(training_data, n_test, model, prior_observations, test_data) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# # define config\n",
    "# config = [24, 500, 100, 100] \n",
    "\n",
    "# # grid search \n",
    "# scores = repeat_evaluate(data, config, n_test)\n",
    "scores = repeat_evaluate(training_data, n_test, model, prior_observations, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5d4045f-c50b-4cec-8767-7fd0cdfd0a7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# summarize scores \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m summarize_scores(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[43mscores\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "# summarize scores \n",
    "summarize_scores('mlp' , scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749de8d6-3edb-49c1-8aaf-da63fb5e4fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
